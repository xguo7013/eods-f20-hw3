{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "### Due: Friday Dec 4th @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will be performing \n",
    "\n",
    "- feature cleaning and engineering\n",
    "\n",
    "- dimensionality reduction\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Follow the comments below and fill in the blanks (\\_\\_\\_\\_) to complete.\n",
    "\n",
    "Please 'Restart and Run All' prior to submission.\n",
    "\n",
    "Where not specified, please run functions with default argument settings.\n",
    "\n",
    "Out of 60 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. (1pt) Set up our environment with comman libraries and plotting.\n",
    "#    Note: generally we would do all of our imports here but some imports\n",
    "#    have been left till later where they are used.\n",
    "\n",
    "# Import numpy, pandas, matplotlib.pyplot and seaborn\n",
    "____\n",
    "\n",
    "# Execute the matplotlib magic function to display plots inline\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be loading, cleaning and transforming a small set of data related to loan applications.\n",
    "\n",
    "There are two files, one containing loan application information and the other containing borrower information.\n",
    "\n",
    "You will need to load both files, join them and then transform this data, creating a new dataframe with features which could then be used for modeling.\n",
    "\n",
    "Each step is followed by a print or plot of some kind to help us catch errors as they happen instead of later in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (2pts) Load Loan Application Data\n",
    "\n",
    "# Read in the first dataframe containing loan application information.\n",
    "# The path to the datafile is '../data/hw3_loan.csv'.\n",
    "# Use the appropriate pandas command to read a csv file.\n",
    "# 'CustomerID' is a unique id that should be set as the index using the index_col argument.\n",
    "# Store this dataframe as df_loan.\n",
    "____\n",
    "\n",
    "# Print the .info() of df_loan and check the size (should be 663 rows, 4 columns)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (2pts) Load Borrower Data\n",
    "\n",
    "# Read in the first dataframe containing borrower information.\n",
    "# The path to the datafile is '../data/hw3_borrower.csv'.\n",
    "# Use the appropriate pandas command to read a csv file.\n",
    "# 'CustomerID' is a unique id that should be set as the index using the index_col argument.\n",
    "# Store this dataframe as df_borrower.\n",
    "____\n",
    "\n",
    "# Print the .info() of df_borrower (should be 633 rows, 2 columns)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (2pts) Join Datasets\n",
    "\n",
    "# Join the df_loan and df_borrower and store as df_joined.\n",
    "# Perform an inner join.\n",
    "# Note that since both dataframes share an index, it is convenient to use the 'join()' function.\n",
    "____\n",
    "\n",
    "# Print df information summary using 'info'\n",
    "# There should still be 663 rows but now 6 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (1pt) LoanReason\n",
    "\n",
    "# \"LoanReason\" is a categorical variable in df_joined.\n",
    "# Print the counts of each category using 'value_counts'\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (2pts) Transform LoanReason Using One-Hot Encoding\n",
    "\n",
    "# Transform just the LoanReason column into one-hot encoding using 'pd.get_dummies()'.\n",
    "# Use prefix='LoanReason' to prefix the column names get_dummies().\n",
    "# Leave all other arguments at defaults.\n",
    "# Store the resulting dataframe as df_loanreason\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_loanreason to confirm the transformation.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (2pts) Create Transformed Feature Dataframe\n",
    "\n",
    "# We are performing these transformations in order to use this data for modelling.\n",
    "#\n",
    "# Instead of adding transformed features into our original dataframe (df_joined)\n",
    "#   it is useful to create a new dataframe containing only features used for modeling.\n",
    "\n",
    "# Create this new dataframe df_features by copying df_loanreason into df_features using .copy().\n",
    "____\n",
    "\n",
    "# Print df_features information summary using 'info'\n",
    "# We should see the 4 columns created for LoanReason\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (1pt) LoanPayoffPeriodInMonths\n",
    "\n",
    "# Use seaborn histplot to plot df_joined.LoanPayoffPeriodInMonths using default settings.\n",
    "# Note that there appear to be several modes in the data corresponding to years.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (1pt) Create Period Bins\n",
    "\n",
    "# We'll bin LoanPayoffPeriodInMonths into [less than 1 year, 1 up to 2 years, 2 or more years]\n",
    "# Create a list with four values\n",
    "#   minimum value in LoanPayoffPeriodInMonths\n",
    "#   12\n",
    "#   24\n",
    "#   maximum value of LoanPayoffPeriodInMonths\n",
    "# Store this list as 'period_bins'\n",
    "____\n",
    "\n",
    "# Print period_bins.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. (3pts) Bin LoanPayoffPeriodInMonths\n",
    "\n",
    "# Use pd.cut() to bin LoanPayoffPeriodInMonths.\n",
    "# Use the period_bins list we created above for the bin edges.\n",
    "# Set include_lowest to True to include the minimum value in the first bin.\n",
    "# Set the bin labels as ['0','1','2+'].\n",
    "# Store as loanperiod_years\n",
    "____\n",
    "\n",
    "# loanperiod_years is a Series.\n",
    "# Print the number of items in each bin of loanperiod_years using .value_counts().\n",
    "# Sort by the index labels (the bin labels) by calling .sort_index() on the result of .value_counts()\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. (2pts) Transform Period Year Bins using One-Hot Encoding\n",
    "\n",
    "# Use 'pd.get_dummies' to encode loanperiod_years.\n",
    "# Use prefix 'LoanPeriodYears'.\n",
    "# Store as df_loanperiod.\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_loanperiod to confirm the transformation.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. (2pts) Extend Transformed Features\n",
    "\n",
    "# Join the existing df_features dataframe with df_loanperiod.\n",
    "# Note that they share an index, so use .join() here.\n",
    "# Store the result back into df_features.\n",
    "____\n",
    "\n",
    "# Print df_features information summary using 'info'.\n",
    "# Note that the new columns have been joined.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. (1pt) RequestedAmount\n",
    "\n",
    "# Use seaborn histplot to plot df_joined.RequestedAmount using default settings.\n",
    "# Note that this features is right skewed and has a very wide range.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. (2pts) Log Transform RequestedAmount\n",
    "\n",
    "# Using the .apply(), apply np.log to the RequestedAmount column.\n",
    "# Store the result as requestedamount_log\n",
    "____\n",
    "\n",
    "# Use seaborn histplot to plot requestedamount_log using default settings.\n",
    "# Note that the shape is is closer to a normal distribution\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. (3pts) Center and Scale log(RequestedAmount) Manually\n",
    "\n",
    "# Standardize requestedamount_log by \n",
    "#   subtracting the mean of the series and dividing by the standard deviation of the series.\n",
    "# Store the result into df_features as column 'RequestedAmount_log_scaled'\n",
    "____\n",
    "\n",
    "# Confirm that scaling has been applied properly by printing out \n",
    "# the mean and std. dev. of RequestedAmount_log_scaled\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. (2pts) Age\n",
    "\n",
    "# The df_joined.Age variable has missing values.\n",
    "# Before we fill the missing values, create a dummy column noting where data is missing.\n",
    "# We want to store this as an int instead of a boolean.\n",
    "# Use .isna().astype(int) on the Age column to both find nulls and convert boolean to int.\n",
    "# Store in df_features as 'Age_missing'.\n",
    "____\n",
    "\n",
    "# Print the number of 0s and 1s in Age_missing using .value_counts().\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. (2pts) Fill Age with Median\n",
    "\n",
    "# Age may be skewed so fill missing values using median instead of mean.\n",
    "# Use fillna() with A to fill the missing values in Age with the median of Age.\n",
    "# Store back into df_joined['Age_filled']\n",
    "____\n",
    "\n",
    "# Use assert and the sum of .isna() to check that there are no missing values in Age_filled\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. (3pts) Center and Scale Age Using StandardScaler\n",
    "\n",
    "# Import StandardScaler from sklearn\n",
    "____\n",
    "\n",
    "# Using StandardScaler and fit_transform, standardize the Age_filled column.\n",
    "# Note that fit_transform expects a DataFrame not a Series.\n",
    "# Use dfjoined[['Age_filled']] to return a DataFrame.\n",
    "# Store the result in df_features as 'Age_scaled'\n",
    "# NOTE: while there are other transformations we might apply to Age (eg log) we'll skip them in this homework\n",
    "____\n",
    "\n",
    "# Print out the mean and standard deviation of Age_scaled with a precision of 2\n",
    "print(f'Age_scaled mean: {____}')\n",
    "print(f'Age_scaled std:  {____}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. (1pt) YearsAtCurrentEmployer\n",
    "\n",
    "# There are missing values in df_joined.YearsAtCurrentEmployer as well.\n",
    "# NOTE: Normally we would want to capture the missing values using a dummy column first.\n",
    "#       We are skipping this step for this homework.\n",
    "# Since this is a categorical feature, we'll fill with the most common value (mode).\n",
    "# First, print the number of items in each category, including nan's\n",
    "#   using value_counts with dropna=False\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. (2pts) Get Mode of YearsAtCurrentEmployer\n",
    "\n",
    "# Pandas Series has a 'mode' function that returns another series containing the modes of the original series.\n",
    "# We just want the first value in that series.\n",
    "# Use .mode().values[0] to get the first value in the series returned by mode.\n",
    "# Store in years_mode\n",
    "____\n",
    "\n",
    "# Print the value found.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. (2pts) Fill Missing in YearsAtCurrentEmployer With Mode\n",
    "\n",
    "# Use fillna and years_mode to fill the missing values in the YearsAtCurrentEmployer column.\n",
    "# Store back into df_joined as YearsAtCurrentEmployer_filled\n",
    "____\n",
    "\n",
    "# Print the value_counts of YearsAtCurrentEmployer_filled, again with dropna=False.\n",
    "# Note that there are no longer nan's.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. (2pts) One-Hot Encode YearsAtCurrentEmployer\n",
    "\n",
    "# Use 'pd.get_dummies' to encode YearsAtCurrentEmployer_filled.\n",
    "# Use prefix 'YearsAtCurrentEmployer'.\n",
    "# Store as df_employed.\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_employed to confirm the transformation.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. (2pts) Extend Transformed Features with YearsAtCurrentEmployer\n",
    "\n",
    "# Join the existing df_features dataframe with df_employed.\n",
    "# Store the result back into df_features.\n",
    "____\n",
    "\n",
    "# Display df_features information summary using 'info'.\n",
    "# Note that the new columns have been joined, all datatypes are numeric and there are no missing values.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. (1pts) Split Data\n",
    "\n",
    "# The target we're interested in predicting is df_joined.WasTheLoanApproved.\n",
    "# This is a categorical variable taking the values Y for yes and N for no\n",
    "\n",
    "# Transform the target df_joined.WasTheLoanApproved\n",
    "#    into integers 0 for N and 1 for Y using a boolean mask and astype(int)\n",
    "# Store in y\n",
    "____\n",
    "\n",
    "# Before we continue we should split up our data into a train and test set\n",
    "\n",
    "# import train_test_split from sklearn\n",
    "____\n",
    "\n",
    "# Generate a training and test set from df_features and y\n",
    "#   with 10% of observations in test, stratify=y, and random_state=42\n",
    "# Store in X_train,X_test,y_train,y_test\n",
    "____\n",
    "\n",
    "# Print the shape of X_train (should be 596 rows, 15 columns).\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23. (6pts) Use a Random Forest Classifier to rank the features in df_features by their importance\n",
    "\n",
    "# Import RandomForestClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Intantiate a RandomForestClassifier object\n",
    "# Use n_estimators=10, random_state=123, n_jobs=-1 and all other arguments as their default.\n",
    "# Store as 'rfc'.\n",
    "____\n",
    "\n",
    "# Fit rfc on the training set\n",
    "____\n",
    "\n",
    "# The feature_importances_ stored in rfc are in the order of the columns of df_features\n",
    "# Store the column names from df_joined as feature_names\n",
    "____\n",
    "\n",
    "# To easily display the column names and importances together\n",
    "#    create a new dataframe called df_feature_importance\n",
    "____\n",
    "\n",
    "# Store feature_names in df_feature_importance with column name \"Feature\"\n",
    "____\n",
    "\n",
    "# Store the feature_importances_ from rf in df_feature_importance as \"Importance\"\n",
    "____\n",
    "\n",
    "# Display df_feature_importance sorted by Importance descending\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. (3pts) Feature Selection with SelectFromModel\n",
    "\n",
    "# Import SelectFromModel from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a SelectFromModel transformer \n",
    "#   with rfc as the estimator and prefit=True (all other arguments default) \n",
    "#   recall that by default, only features with importances greater than the mean are returned\n",
    "____\n",
    "\n",
    "# Display the selected features using feature_names and sfm.get_support()\n",
    "# The should be the top 2 features listed in the previous cell\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. (2pts) Transform Data Using Selected Features\n",
    "\n",
    "# Create a new dataset using only the features selected in the previous step.\n",
    "# Use sfm to transform X_train and store as X_train_fs\n",
    "____\n",
    "\n",
    "# Use sfm to transform X_test and store as X_test_fs\n",
    "____\n",
    "\n",
    "# Print the shape of X_train_fs (should be 596 rows, 2 columns).\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. (2pts) Train and Evaluate Model On Selected Features\n",
    "\n",
    "# Instantiate a new RandomForestClassifier\n",
    "#   with n_estimators=50, max_depth=5 and n_jobs=-1\n",
    "# Store in rfc_fs\n",
    "____\n",
    "\n",
    "# Train the rfc_fs model on X_train_fs and y_train\n",
    "____\n",
    "\n",
    "# Print the accuracy achieved by rfc_fs on both \n",
    "#   the training (X_train_fs,y_train) and test set (X_test_fs,y_test) with precision of 2 decimal places\n",
    "print(f'training accuracy: {____}')\n",
    "print(f'test accuracy    : {____}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. (2pts) Reduce Dataset to 3D Using PCA\n",
    "\n",
    "# Import PCA from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a pca object that will result in 3 components being returned and random_state=123\n",
    "# Store as 'pca'.\n",
    "____\n",
    "\n",
    "# Fit and transform the full training set (X_train) to 3d using pca\n",
    "# Store in X_train_pca\n",
    "____\n",
    "\n",
    "# Transform the full test set (X_test) to 3d using pca\n",
    "# Store in X_test_pca\n",
    "____\n",
    "\n",
    "# Print the proportion of variance explained by each component \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. (1pts) Plot the First 2 Dimensions of the PCA Transformation\n",
    "\n",
    "# Use sns.scatterplot to plot the PCA transformed training set\n",
    "#   with the first column on the x-axis and the second column \n",
    "#   colored by the target y_train (using the hue parameter)\n",
    "____\n",
    "\n",
    "# Note that the data categories are still very mixed.\n",
    "# Our models will have a difficult time with the data as is.\n",
    "# Additional features and feature engineering would be needed for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f20",
   "language": "python",
   "name": "eods-f20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
